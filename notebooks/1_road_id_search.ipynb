{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhpdd/geneal_tsp_150subang/blob/main/notebooks/1_road_id_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required package\n",
        "!pip install osmnx\n",
        "\n",
        "# Standard library imports\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "import folium\n",
        "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
        "from shapely.ops import unary_union, polygonize\n",
        "from geopy.distance import great_circle\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ebmCbm0YsOnp"
      },
      "id": "ebmCbm0YsOnp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b341d780",
      "metadata": {
        "id": "b341d780"
      },
      "outputs": [],
      "source": [
        "# Get all values from the worksheet and convert them into a pandas DataFrame\n",
        "df = pd.read_excel('/content/drive/MyDrive/Colab/Capstone 1/tprop_df_validated.xlsx', sheet_name = 'test_1_static')\n",
        "\n",
        "# Step 4: Display the DataFrame's info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all values from the worksheet and convert them into a pandas DataFrame\n",
        "df = pd.read_excel('/content/drive/MyDrive/Colab/Capstone 1/tprop_df_test2_updated.xlsx', sheet_name = 'test2')\n",
        "\n",
        "# Step 4: Display the DataFrame's info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "_WtGh8qoF6vV"
      },
      "id": "_WtGh8qoF6vV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "dsfTNroB8k17"
      },
      "id": "dsfTNroB8k17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['road_name'].nunique()"
      ],
      "metadata": {
        "id": "E-5NJAfTWM-c"
      },
      "id": "E-5NJAfTWM-c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2"
      ],
      "metadata": {
        "id": "0SmaFI1cDWh1"
      },
      "id": "0SmaFI1cDWh1"
    },
    {
      "cell_type": "code",
      "source": [
        "OSM_API_BASE_URL = \"https://api.openstreetmap.org/api/0.6\"\n",
        "\n",
        "# Define your headers once\n",
        "HEADERS = {\n",
        "    'User-Agent': 'MyDataProject/1.0 (https://example.com; myemail@example.com)'\n",
        "}\n",
        "\n",
        "def fetch_osm_data(url: str, timeout: int = 25) -> ET.Element | None:\n",
        "    \"\"\"\n",
        "    Fetches data from the OSM API and parses it as XML.\n",
        "    Includes a required User-Agent header.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add the headers to your request\n",
        "        response = requests.get(url, timeout=timeout, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return ET.fromstring(response.content)\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # Your excellent 404 handling\n",
        "        if e.response is not None and e.response.status_code == 404:\n",
        "            return None\n",
        "        print(f\"HTTP Error for {url}: {e}\")\n",
        "        return None # Explicitly return None on other HTTP errors\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {e}\")\n",
        "        return None # Explicitly return None on failure\n",
        "\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"XML parsing failed for {url}. Error: {e}\")\n",
        "        return None # Explicitly return None on failure\n",
        "\n",
        "\n",
        "def extract_line_geometry(input_df: pd.DataFrame, name_column: str, id_column: str) -> Optional[gpd.GeoDataFrame]:\n",
        "    \"\"\"\n",
        "    Fetches OSM data for Way IDs, parses road geometry, and returns a GeoDataFrame.\n",
        "    Includes detailed print statements and a progress bar to show the process.\n",
        "\n",
        "    Args:\n",
        "        input_df (pd.DataFrame): DataFrame with a column for Way IDs.\n",
        "        name_column (str): The name of the column containing the road/location name.\n",
        "        id_column (str): The name of the column containing the OSM Way ID.\n",
        "\n",
        "    Returns:\n",
        "        Optional[gpd.GeoDataFrame]: A GeoDataFrame with road geometries and\n",
        "                                    merged attributes, or None if no valid data.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Geometry Extraction Process ---\")\n",
        "    all_geometries = []\n",
        "    all_attributes = []\n",
        "\n",
        "    # Using tqdm to create a progress bar for the loop\n",
        "    for index, row in tqdm(input_df.iterrows(), total=len(input_df), desc=\"Processing Way IDs\"):\n",
        "        way_id = str(row[id_column])\n",
        "\n",
        "        print(f\"\\nProcessing Way ID: {way_id} for '{row[name_column]}'\")\n",
        "\n",
        "        url = f\"{OSM_API_BASE_URL}/way/{way_id}/full\"\n",
        "        print(f\"  -> Fetching data from: {url}\")\n",
        "        root = fetch_osm_data(url)\n",
        "\n",
        "        if not root:\n",
        "            print(f\"  -> Skipping Way ID '{way_id}' due to data fetch/parse error.\")\n",
        "            continue\n",
        "\n",
        "        road_way_elem = root.find(f\".//way[@id='{way_id}']\")\n",
        "        if not road_way_elem:\n",
        "            print(f\"  -> WARNING: Road Way ID '{way_id}' not found in the fetched XML. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Cache all node coordinates from the current XML response\n",
        "        node_coords_cache = {}\n",
        "        for node_elem in root.findall(\".//node\"):\n",
        "            try:\n",
        "                node_id = node_elem.get('id')\n",
        "                lat = float(node_elem.get('lat'))\n",
        "                lon = float(node_elem.get('lon'))\n",
        "                node_coords_cache[node_id] = (lon, lat)\n",
        "            except (TypeError, ValueError):\n",
        "                pass # Skip nodes with invalid data\n",
        "\n",
        "        print(f\"  -> Found {len(node_coords_cache)} nodes in the XML.\")\n",
        "\n",
        "        # Extract road attributes (tags) from OSM\n",
        "        osm_road_attrs = {'id': way_id}\n",
        "        for tag in road_way_elem.findall('tag'):\n",
        "            osm_road_attrs[tag.get('k')] = tag.get('v')\n",
        "\n",
        "        # Combine attributes from the input DataFrame row with OSM attributes\n",
        "        merged_attrs = row.to_dict()\n",
        "        merged_attrs.update(osm_road_attrs)\n",
        "\n",
        "        # Build the coordinate list for the LineString geometry\n",
        "        road_coords = []\n",
        "        for nd_ref_elem in road_way_elem.findall('nd'):\n",
        "            node_ref = nd_ref_elem.get('ref')\n",
        "            coords = node_coords_cache.get(node_ref)\n",
        "            if coords:\n",
        "                road_coords.append(coords)\n",
        "\n",
        "        # Create the LineString if we have at least two points\n",
        "        if len(road_coords) >= 2:\n",
        "            line_geometry = LineString(road_coords)\n",
        "            all_geometries.append(line_geometry)\n",
        "            all_attributes.append(merged_attrs)\n",
        "            print(f\"  -> SUCCESS: Created LineString for Way ID '{way_id}' with {len(road_coords)} points.\")\n",
        "        else:\n",
        "            print(f\"  -> WARNING: Insufficient coordinates for Way ID '{way_id}'. Skipping.\")\n",
        "\n",
        "    if not all_geometries:\n",
        "        print(\"\\n--- Process Finished: No valid geometries could be created. ---\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n--- Finalizing GeoDataFrame ---\")\n",
        "    print(f\"Creating GeoDataFrame with {len(all_geometries)} geometries...\")\n",
        "    gdf = gpd.GeoDataFrame(all_attributes, geometry=all_geometries, crs=\"EPSG:4326\")\n",
        "\n",
        "    print(f\"Dropping duplicate entries based on the '{name_column}' column...\")\n",
        "    gdf = gdf.drop_duplicates(subset=[name_column])\n",
        "    print(f\"  -> Rows after dropping duplicates: {len(gdf)}\")\n",
        "\n",
        "    print(\"Filtering final columns...\")\n",
        "    gdf = gdf[[name_column, 'id', 'geometry']]\n",
        "\n",
        "    print(\"--- Geometry Extraction Complete ---\")\n",
        "    return gdf\n",
        "\n",
        "def generate_geometry_summary(\n",
        "    result_gdf: Optional[gpd.GeoDataFrame],\n",
        "    original_df: pd.DataFrame,\n",
        "    name_column: str\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Analyzes the result of the geometry extraction and prints a summary.\n",
        "\n",
        "    Compares the final GeoDataFrame against the initial DataFrame to determine\n",
        "    success and failure rates for creating road geometries.\n",
        "\n",
        "    Args:\n",
        "        result_gdf (Optional[gpd.GeoDataFrame]): The GeoDataFrame returned by the\n",
        "                                                 extract_line_geometry function.\n",
        "                                                 Can be None if the process failed entirely.\n",
        "        original_df (pd.DataFrame): The original DataFrame that was passed to the\n",
        "                                    geometry extraction function.\n",
        "        name_column (str): The name of the column containing the unique road/location names.\n",
        "\n",
        "    Returns:\n",
        "        Optional[pd.DataFrame]: A DataFrame containing the rows of the original data\n",
        "                                that failed to produce a valid geometry, or None if all\n",
        "                                were successful.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Geometry Extraction Summary ---\")\n",
        "\n",
        "    if original_df.empty:\n",
        "        print(\"Original DataFrame is empty. No roads to process.\")\n",
        "        return None\n",
        "\n",
        "    total_unique_roads = original_df[name_column].nunique()\n",
        "    print(f\"Total unique roads attempted: {total_unique_roads}\")\n",
        "\n",
        "    if result_gdf is None or result_gdf.empty:\n",
        "        print(\"No geometries were successfully created.\")\n",
        "        print(f\"Success rate: 0.00%\")\n",
        "        print(f\"Failure rate: 100.00%\")\n",
        "        return original_df # All records failed\n",
        "\n",
        "    successful_count = len(result_gdf)\n",
        "    failed_count = total_unique_roads - successful_count\n",
        "\n",
        "    print(f\"Successfully created geometries: {successful_count} roads ({successful_count/total_unique_roads:.2%})\")\n",
        "    print(f\"Failed to create geometries: {failed_count} roads ({failed_count/total_unique_roads:.2%})\")\n",
        "\n",
        "    # Identify which specific roads failed\n",
        "    successful_names = set(result_gdf[name_column])\n",
        "    original_names = set(original_df[name_column])\n",
        "    failed_names = original_names - successful_names\n",
        "\n",
        "    if failed_names:\n",
        "        print(f\"\\nReturning a DataFrame with {len(failed_names)} failed records for review.\")\n",
        "        failed_df = original_df[original_df[name_column].isin(failed_names)].copy()\n",
        "        return failed_df\n",
        "    else:\n",
        "        print(\"\\nCongratulations! All road geometries were created successfully.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "bnlKBmL5DsjE"
      },
      "id": "bnlKBmL5DsjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "road_df = df[df['test2_result'] == 'Failed'][['road_name', 'id']].drop_duplicates().reset_index(drop=True)\n",
        "road_df"
      ],
      "metadata": {
        "id": "vnxRD8rJEU9w"
      },
      "id": "vnxRD8rJEU9w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the road route details\n",
        "road_route_details = extract_line_geometry(road_df, 'road_name','id').reset_index(drop=True)\n",
        "road_route_details"
      ],
      "metadata": {
        "id": "gjlEvsMfDbyh"
      },
      "id": "gjlEvsMfDbyh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_geometry_summary(road_route_details, road_df, 'road_name')"
      ],
      "metadata": {
        "id": "baZ0GDe4H-Rc"
      },
      "id": "baZ0GDe4H-Rc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replaces 'old_column_name' with 'way_id'\n",
        "road_route_details['id'] = road_route_details['id'].astype(int)"
      ],
      "metadata": {
        "id": "jb_6jRGxG4Nh"
      },
      "id": "jb_6jRGxG4Nh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "road_route_details.head()"
      ],
      "metadata": {
        "id": "I8bef2G4gewg"
      },
      "id": "I8bef2G4gewg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tprop_df2 = df.merge(road_route_details, on =['road_name', 'id'], how='left')"
      ],
      "metadata": {
        "id": "1hBcHcHrGxHf"
      },
      "id": "1hBcHcHrGxHf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tprop_df2['test2_result'] = np.where(tprop_df2['geometry'].notna(), 'Pass', 'Failed')\n",
        "tprop_df2[(tprop_df2['test2_result'] == 'Failed')]"
      ],
      "metadata": {
        "id": "SNWTZFfDgTNZ"
      },
      "id": "SNWTZFfDgTNZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tprop_df2.to_excel('/content/drive/MyDrive/Colab/Capstone 1/tprop_df_test2_updated.xlsx')"
      ],
      "metadata": {
        "id": "azNqvtDLhDrH"
      },
      "id": "azNqvtDLhDrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 3"
      ],
      "metadata": {
        "id": "N095iWBtDcFi"
      },
      "id": "N095iWBtDcFi"
    },
    {
      "cell_type": "code",
      "source": [
        "tprop_df2 = pd.read_excel('/content/drive/MyDrive/Colab/Capstone 1/tprop_df_test2_updated.xlsx')\n",
        "tprop_df2"
      ],
      "metadata": {
        "id": "KNW5dnd5-Wu6"
      },
      "id": "KNW5dnd5-Wu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tprop_df2.info()"
      ],
      "metadata": {
        "id": "IKIBx3OJA-pe"
      },
      "id": "IKIBx3OJA-pe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely import wkt\n",
        "\n",
        "# Step 1: Check current status\n",
        "print(f\"Total rows: {len(tprop_df2)}\")\n",
        "print(f\"Missing geometries: {tprop_df2['geometry'].isna().sum()}\")\n",
        "\n",
        "# Step 2: Prepare the data - clean the ID column\n",
        "missing_mask = tprop_df2['geometry'].isna()\n",
        "\n",
        "if missing_mask.any():\n",
        "    missing_geom_df = tprop_df2[missing_mask].copy()\n",
        "\n",
        "    # Filter out rows with NaN IDs and convert to proper integers\n",
        "    valid_id_mask = missing_geom_df['id'].notna()\n",
        "    missing_geom_df = missing_geom_df[valid_id_mask].copy()\n",
        "\n",
        "    # Convert IDs from float to integer (removes the .0)\n",
        "    missing_geom_df['id'] = missing_geom_df['id'].astype(int).astype(str)\n",
        "\n",
        "    print(f\"\\nValid rows to process: {len(missing_geom_df)}\")\n",
        "    print(f\"Rows with NaN IDs (skipped): {(~valid_id_mask).sum()}\")\n",
        "\n",
        "    if len(missing_geom_df) > 0:\n",
        "        print(f\"\\nExtracting geometries for {len(missing_geom_df)} rows...\")\n",
        "\n",
        "        # Step 3: Extract geometries from OSM\n",
        "        extracted_gdf = extract_line_geometry(\n",
        "            input_df=missing_geom_df,\n",
        "            name_column='road_name',\n",
        "            id_column='id'\n",
        "        )\n",
        "\n",
        "        # Step 4: Fill missing geometries\n",
        "        if extracted_gdf is not None and not extracted_gdf.empty:\n",
        "            # Directly update geometries using iterrows (preserves Shapely objects)\n",
        "            for idx, row in extracted_gdf.iterrows():\n",
        "                road_name = row['road_name']\n",
        "                geometry = row['geometry']\n",
        "\n",
        "                # Find matching rows and update geometry\n",
        "                mask = (tprop_df2['road_name'] == road_name) & (tprop_df2['geometry'].isna())\n",
        "                if mask.any():\n",
        "                    tprop_df2.loc[mask, 'geometry'] = geometry\n",
        "\n",
        "            filled_count = len(extracted_gdf)\n",
        "            print(f\"\\n✓ Successfully filled {filled_count} geometries\")\n",
        "            print(f\"Remaining missing geometries: {tprop_df2['geometry'].isna().sum()}\")\n",
        "        else:\n",
        "            print(\"\\n✗ No geometries could be extracted\")\n",
        "\n",
        "        # Step 5: Show summary\n",
        "        failed_df = generate_geometry_summary(extracted_gdf, missing_geom_df, 'road_name')\n",
        "\n",
        "        if failed_df is not None:\n",
        "            print(f\"\\n⚠ {len(failed_df)} roads failed to extract:\")\n",
        "            display(failed_df[['road_name', 'id']].head(10))\n",
        "    else:\n",
        "        print(\"\\n⚠ No valid IDs to process (all are NaN)\")\n",
        "else:\n",
        "    print(\"✓ No missing geometries to fill!\")\n",
        "\n",
        "# Step 6: Convert string geometries to Shapely objects BEFORE creating GeoDataFrame\n",
        "print(\"\\nConverting geometry column to Shapely objects...\")\n",
        "\n",
        "def convert_to_shapely(geom):\n",
        "    \"\"\"Convert string WKT or keep Shapely geometry.\"\"\"\n",
        "    if pd.isna(geom):\n",
        "        return None\n",
        "    elif isinstance(geom, str):\n",
        "        try:\n",
        "            return wkt.loads(geom)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not parse geometry: {geom[:50]}...\")\n",
        "            return None\n",
        "    else:\n",
        "        # Already a Shapely geometry\n",
        "        return geom\n",
        "\n",
        "tprop_df2['geometry'] = tprop_df2['geometry'].apply(convert_to_shapely)\n",
        "\n",
        "# Step 7: Now convert to GeoDataFrame\n",
        "if not isinstance(tprop_df2, gpd.GeoDataFrame):\n",
        "    tprop_df2 = gpd.GeoDataFrame(tprop_df2, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"FINAL SUMMARY:\")\n",
        "print(f\"Total rows: {len(tprop_df2)}\")\n",
        "print(f\"Rows with geometry: {tprop_df2['geometry'].notna().sum()}\")\n",
        "print(f\"Rows without geometry: {tprop_df2['geometry'].isna().sum()}\")\n",
        "print(f\"✓ Successfully converted to GeoDataFrame!\")"
      ],
      "metadata": {
        "id": "oiPG_0MO_sTi"
      },
      "id": "oiPG_0MO_sTi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OSM_API_BASE_URL = \"https://api.openstreetmap.org/api/0.6\"\n",
        "\n",
        "# Define your headers once\n",
        "HEADERS = {\n",
        "    'User-Agent': 'MyDataProject/1.0 (https://example.com; myemail@example.com)'\n",
        "}\n",
        "\n",
        "def fetch_osm_data(url: str, timeout: int = 25) -> ET.Element | None:\n",
        "    \"\"\"\n",
        "    Fetches data from the OSM API and parses it as XML.\n",
        "    Includes a required User-Agent header.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add the headers to your request\n",
        "        response = requests.get(url, timeout=timeout, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return ET.fromstring(response.content)\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # Your excellent 404 handling\n",
        "        if e.response is not None and e.response.status_code == 404:\n",
        "            return None\n",
        "        print(f\"HTTP Error for {url}: {e}\")\n",
        "        return None # Explicitly return None on other HTTP errors\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {e}\")\n",
        "        return None # Explicitly return None on failure\n",
        "\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"XML parsing failed for {url}. Error: {e}\")\n",
        "        return None # Explicitly return None on failure"
      ],
      "metadata": {
        "id": "zAJM7TZ3hNRK"
      },
      "id": "zAJM7TZ3hNRK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_district_data(object_id: str) -> dict | None:\n",
        "    \"\"\"\n",
        "    Fetches OSM data for a given object_id, trying as relation, then way, then node.\n",
        "    Correctly handles inner and outer ways for relations to form polygons with holes.\n",
        "    \"\"\"\n",
        "    final_object_type = None\n",
        "    final_tags = {}\n",
        "    final_name_tag = None\n",
        "    final_all_polygons_coordinates = []\n",
        "    processed_successfully = False\n",
        "\n",
        "    # --- Try to process as a RELATION ---\n",
        "    try:\n",
        "        relation_url = f\"{OSM_API_BASE_URL}/relation/{object_id}/full\"\n",
        "        root_xml = fetch_osm_data(relation_url)\n",
        "        if not root_xml:\n",
        "            raise ValueError(\"XML data could not be fetched.\")\n",
        "\n",
        "        relation_element = root_xml.find(f\".//relation[@id='{object_id}']\")\n",
        "        if relation_element is None:\n",
        "            raise ValueError(\"Relation element not found in XML.\")\n",
        "\n",
        "        # (This part for getting tags and caching node coordinates is correct and remains the same)\n",
        "        current_tags = {tag.get('k'): tag.get('v') for tag in relation_element.findall('tag') if tag.get('k')}\n",
        "        current_name_tag = current_tags.get('name')\n",
        "        nodes_coords_cache = {\n",
        "            node.get('id'): (float(node.get('lon')), float(node.get('lat')))\n",
        "            for node in root_xml.findall('.//node') if node.get('id') and node.get('lat') and node.get('lon')\n",
        "        }\n",
        "\n",
        "        # (This part for separating ways into outer/inner segments is also correct)\n",
        "        outer_way_segments = []\n",
        "        inner_way_segments = []\n",
        "        for member in relation_element.findall(\"member[@type='way']\"):\n",
        "            way_elem = root_xml.find(f\".//way[@id='{member.get('ref')}']\")\n",
        "            if way_elem is not None:\n",
        "                coords = [nodes_coords_cache[nd.get('ref')] for nd in way_elem.findall('nd') if nd.get('ref') in nodes_coords_cache]\n",
        "                if coords:\n",
        "                    role = member.get('role', 'outer')\n",
        "                    if role == 'outer':\n",
        "                        outer_way_segments.append(coords)\n",
        "                    elif role == 'inner':\n",
        "                        inner_way_segments.append(coords)\n",
        "\n",
        "        # --- DELETED SECTION: The original, fragile stitching loops have been removed. ---\n",
        "\n",
        "        # +++ NEW, ROBUST STITCHING AND POLYGON CREATION LOGIC +++\n",
        "\n",
        "        # Convert coordinate segments into Shapely LineString objects\n",
        "        outer_lines = [LineString(segment) for segment in outer_way_segments]\n",
        "        inner_lines = [LineString(segment) for segment in inner_way_segments]\n",
        "\n",
        "        # Merge all connecting lines into continuous, single paths\n",
        "        merged_outer_lines = unary_union(outer_lines)\n",
        "        merged_inner_lines = unary_union(inner_lines)\n",
        "\n",
        "        # Form valid polygons from the closed rings created by the merged lines\n",
        "        stitched_outer_polygons = list(polygonize(merged_outer_lines))\n",
        "        stitched_inner_polygons = list(polygonize(merged_inner_lines))\n",
        "\n",
        "        # --- RESUMING LOGIC WITH CORRECTLY FORMED POLYGONS ---\n",
        "\n",
        "        final_shapely_polygons = []\n",
        "        # Create a mutable list of inner polygons to track which ones have been used\n",
        "        remaining_inners = list(stitched_inner_polygons)\n",
        "\n",
        "        for outer_poly in stitched_outer_polygons:\n",
        "            holes_for_this_poly = []\n",
        "            # This list will hold inner polygons that haven't been assigned to this outer_poly\n",
        "            unassigned_inners = []\n",
        "\n",
        "            for inner_poly in remaining_inners:\n",
        "                # Check if the inner polygon is properly contained within the outer one\n",
        "                if outer_poly.contains(inner_poly):\n",
        "                    holes_for_this_poly.append(inner_poly.exterior.coords)\n",
        "                else:\n",
        "                    unassigned_inners.append(inner_poly)\n",
        "\n",
        "            # Update the list of remaining inners for the next outer polygon\n",
        "            remaining_inners = unassigned_inners\n",
        "\n",
        "            # Create the final polygon with its associated holes\n",
        "            final_shapely_polygons.append(Polygon(outer_poly.exterior.coords, holes_for_this_poly))\n",
        "\n",
        "        # --- (The rest of the function for formatting output remains the same) ---\n",
        "\n",
        "        if final_shapely_polygons:\n",
        "            for shp_poly in final_shapely_polygons:\n",
        "                exterior_coords = list(shp_poly.exterior.coords)\n",
        "                if not shp_poly.exterior.is_ccw:\n",
        "                    exterior_coords.reverse()\n",
        "                poly_data = [exterior_coords]\n",
        "                for interior_ring in shp_poly.interiors:\n",
        "                    interior_coords = list(interior_ring.coords)\n",
        "                    if interior_ring.is_ccw:\n",
        "                        interior_coords.reverse()\n",
        "                    poly_data.append(interior_coords)\n",
        "                final_all_polygons_coordinates.append(poly_data)\n",
        "\n",
        "            final_tags = current_tags\n",
        "            final_name_tag = current_name_tag\n",
        "            final_object_type = \"relation\"\n",
        "            processed_successfully = True\n",
        "            print(f\"Successfully processed ID {object_id} as RELATION with {len(final_shapely_polygons)} polygon(s).\")\n",
        "        else:\n",
        "            print(f\"INFO: Relation {object_id} could not form valid polygons.\")\n",
        "\n",
        "    except Exception as e_relation:\n",
        "        print(f\"Error processing relation for {object_id}: {e_relation}. Trying as way.\")\n",
        "        processed_successfully = False\n",
        "\n",
        "    # (The fallback logic for 'WAY' and 'NODE' remains unchanged)\n",
        "    # ...\n",
        "\n",
        "    # --- Final Return ---\n",
        "    if not processed_successfully or not final_all_polygons_coordinates:\n",
        "        print(f\"FINAL: Could not derive usable geometry for OSM object ID {object_id}.\")\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        'name': final_name_tag,\n",
        "        'tags': final_tags,\n",
        "        'all_polygons_coordinates': final_all_polygons_coordinates,\n",
        "        'id': object_id,\n",
        "        'type': final_object_type\n",
        "    }"
      ],
      "metadata": {
        "id": "3T9bQAvahYlb"
      },
      "id": "3T9bQAvahYlb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_geometry(row):\n",
        "    coords_list = row['all_polygons_coordinates'] # This will be a list of lists of coords\n",
        "    if not coords_list:\n",
        "        return None\n",
        "\n",
        "    if row['type'] == 'node':\n",
        "        # For a node, coords_list will contain one list with one (lon, lat) tuple: [[(lon, lat)]]\n",
        "        if coords_list and len(coords_list[0]) == 1:\n",
        "            return Point(coords_list[0][0])\n",
        "        return None\n",
        "\n",
        "    # --- NEW / MODIFIED BLOCK FOR WAY TYPES ---\n",
        "    elif row['type'] == 'way_polygon': # This is the new type from get_district_data\n",
        "        # It's already determined as a closed way with area tags\n",
        "        # coords_list will contain one list of (lon, lat) tuples for the polygon exterior\n",
        "        if coords_list and coords_list[0] and len(coords_list[0]) >= 4: # Min 3 unique points + 1 closing = 4\n",
        "            # The get_district_data function should have already ensured it's closed,\n",
        "            # but a defensive check here doesn't hurt.\n",
        "            if coords_list[0][0] != coords_list[0][-1]:\n",
        "                # This case should ideally not happen if get_district_data is perfect,\n",
        "                # but if it does, try to close it or log a warning.\n",
        "                print(f\"Warning: way_polygon {row['id']} was not closed, closing it now.\")\n",
        "                return Polygon(coords_list[0] + [coords_list[0][0]])\n",
        "            return Polygon(coords_list[0])\n",
        "        else:\n",
        "            # If not enough points for a polygon despite being flagged as 'way_polygon',\n",
        "            # it might be malformed data. Return None or try LineString if appropriate.\n",
        "            print(f\"Warning: way_polygon {row['id']} has insufficient points ({len(coords_list[0])}). Cannot form Polygon.\")\n",
        "            return None # Or LineString(coords_list[0]) if you want a fallback\n",
        "        return None\n",
        "\n",
        "    elif row['type'] == 'way_line': # This is the new type from get_district_data\n",
        "        # It's already determined as a line-string\n",
        "        # coords_list will contain one list of (lon, lat) tuples for the line\n",
        "        if coords_list and coords_list[0] and len(coords_list[0]) >= 2: # Min 2 points for a LineString\n",
        "            return LineString(coords_list[0])\n",
        "        else:\n",
        "            print(f\"Warning: way_line {row['id']} has insufficient points ({len(coords_list[0])}). Cannot form LineString.\")\n",
        "            return None\n",
        "        return None\n",
        "    # --- END NEW / MODIFIED BLOCK ---\n",
        "\n",
        "    elif row['type'] == 'relation':\n",
        "        polygons = []\n",
        "        for poly_data in coords_list: # Each poly_data is [exterior_coords, [hole1_coords], ...]\n",
        "            if poly_data:\n",
        "                exterior_coords = poly_data[0]\n",
        "                interior_coords_list = poly_data[1:]\n",
        "\n",
        "                # Create Shapely Polygon with holes\n",
        "                try:\n",
        "                    poly = Polygon(exterior_coords, interior_coords_list)\n",
        "                    if poly.is_valid:\n",
        "                        polygons.append(poly)\n",
        "                    else:\n",
        "                        print(f\"Warning: Invalid polygon created for relation {row['id']}. Attempting to make valid.\")\n",
        "                        # Attempt to make invalid polygons valid (e.g., using buffer(0))\n",
        "                        valid_poly = poly.buffer(0)\n",
        "                        if valid_poly.is_valid:\n",
        "                            if isinstance(valid_poly, MultiPolygon):\n",
        "                                polygons.extend(valid_poly.geoms)\n",
        "                            else:\n",
        "                                polygons.append(valid_poly)\n",
        "                        else:\n",
        "                            print(f\"Warning: Could not make polygon valid for relation {row['id']}.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating polygon for relation {row['id']}: {e}\")\n",
        "\n",
        "        if polygons:\n",
        "            return MultiPolygon(polygons) if len(polygons) > 1 else polygons[0]\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        # This 'else' block will catch the old 'way' type if it somehow persists,\n",
        "        # or any other unexpected types.\n",
        "        print(f\"Unsupported or old type '{row['type']}'. Skipping geometry creation.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_amen_gdf(df, id_column):\n",
        "    gdf_list = []\n",
        "    # Ensure the id_column in the input DataFrame `df` is treated as string\n",
        "    df[id_column] = df[id_column].astype(str)\n",
        "\n",
        "    # Fetch data for each ID and append to gdf_list\n",
        "    for osm_id in df[id_column].unique(): # Use unique IDs to avoid redundant API calls\n",
        "        result = get_district_data(osm_id)\n",
        "        if result:\n",
        "            gdf_list.append(result)\n",
        "\n",
        "    if not gdf_list:\n",
        "        print(\"No valid district data was fetched.\")\n",
        "        return None\n",
        "\n",
        "    # Create a DataFrame from the fetched OSM data\n",
        "    dff = pd.DataFrame(gdf_list)\n",
        "\n",
        "    # Merge the original DataFrame with the fetched OSM data\n",
        "    # Use left_on=id_column, right_on='id' to merge correctly\n",
        "    merge_df = pd.merge(df, dff, left_on=id_column, right_on='id', how='left')\n",
        "\n",
        "    # Create geometry column\n",
        "    merge_df['geometry'] = merge_df.apply(create_geometry, axis=1)\n",
        "\n",
        "    # Filter out rows where geometry could not be created\n",
        "    merge_df = merge_df[merge_df['geometry'].notnull()].reset_index(drop=True)\n",
        "\n",
        "    # Create GeoDataFrame\n",
        "    gdf = gpd.GeoDataFrame(merge_df, geometry='geometry', crs=\"EPSG:4326\")\n",
        "    return gdf"
      ],
      "metadata": {
        "id": "It1F3QeehaPf"
      },
      "id": "It1F3QeehaPf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "district_osm_dict = {\n",
        "    'GOMBAK': '12438352',\n",
        "    'HULU LANGAT': '12438351',\n",
        "    'KLANG': '12391135',\n",
        "    'HULU SELANGOR': '10714199',\n",
        "    'KUALA LANGAT': '10743362',\n",
        "    'KUALA LUMPUR': '2939672',\n",
        "    'KUALA SELANGOR': '10714137',\n",
        "    'PETALING': '12391134',\n",
        "    'PUTRAJAYA': '4443881',\n",
        "    'SABAK BERNAM': '10714136',\n",
        "    'SEPANG': '10743315'\n",
        "}\n",
        "\n",
        "district_df = pd.DataFrame.from_dict(district_osm_dict, orient='index', columns=['id'])\n",
        "district_df.index.name = 'district'\n",
        "district_df = district_df.reset_index()\n",
        "district_df['id'] = district_df['id'].astype(str) # Ensure IDs are strings\n",
        "\n",
        "display(district_df.head())"
      ],
      "metadata": {
        "id": "luYduXBrI-NZ"
      },
      "id": "luYduXBrI-NZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dis = create_amen_gdf(district_df, 'id')\n",
        "dis"
      ],
      "metadata": {
        "id": "3HjiAhD7hqGy"
      },
      "id": "3HjiAhD7hqGy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_record_locations(\n",
        "    records_gdf: gpd.GeoDataFrame,\n",
        "    districts_gdf: gpd.GeoDataFrame,\n",
        "    district_col: str = 'district'\n",
        ") -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    Finds the actual district for each road (LineString) based on maximum length overlap.\n",
        "\n",
        "    Args:\n",
        "        records_gdf (gpd.GeoDataFrame): Roads to check. Must have LineString geometry and `district_col`.\n",
        "        districts_gdf (gpd.GeoDataFrame): District boundaries with Polygon geometry.\n",
        "        district_col (str): The column name linking records to districts.\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: Original records with 'actual_district', 'is_in_correct_district',\n",
        "                          and 'overlap_length' columns.\n",
        "    \"\"\"\n",
        "    # --- 1. Validation and Preparation ---\n",
        "    if records_gdf.empty or districts_gdf.empty:\n",
        "        print(\"Warning: One or both GeoDataFrames are empty.\")\n",
        "        records_gdf['actual_district'] = 'N/A'\n",
        "        records_gdf['is_in_correct_district'] = False\n",
        "        return records_gdf\n",
        "\n",
        "    # Ensure both GeoDataFrames use the same CRS\n",
        "    if records_gdf.crs != districts_gdf.crs:\n",
        "        print(f\"Warning: CRS mismatch. Reprojecting records_gdf to match districts_gdf.\")\n",
        "        records_gdf = records_gdf.to_crs(districts_gdf.crs)\n",
        "\n",
        "    # --- 2. Validate and Fix District Geometries ---\n",
        "    districts_clean = districts_gdf.copy()\n",
        "    districts_clean['geometry'] = districts_clean['geometry'].buffer(0)  # Fix any invalid geometries\n",
        "\n",
        "    # --- 3. Find District with Maximum Road Length Overlap ---\n",
        "    results = []\n",
        "\n",
        "    for idx, record in records_gdf.iterrows():\n",
        "        road_geom = record.geometry\n",
        "        assigned_district = record[district_col]\n",
        "\n",
        "        best_district = None\n",
        "        max_overlap_length = 0\n",
        "        overlap_details = {}\n",
        "\n",
        "        # Check intersection with each district\n",
        "        for _, district in districts_clean.iterrows():\n",
        "            district_geom = district.geometry\n",
        "            district_name = district[district_col]\n",
        "\n",
        "            if district_geom.intersects(road_geom):\n",
        "                # Calculate the length of road within this district\n",
        "                intersection = district_geom.intersection(road_geom)\n",
        "\n",
        "                # Handle different intersection result types\n",
        "                if intersection.is_empty:\n",
        "                    overlap_length = 0\n",
        "                elif hasattr(intersection, 'length'):\n",
        "                    overlap_length = intersection.length\n",
        "                else:\n",
        "                    # Handle GeometryCollection or MultiLineString\n",
        "                    overlap_length = sum(\n",
        "                        geom.length for geom in intersection.geoms\n",
        "                        if hasattr(geom, 'length')\n",
        "                    )\n",
        "\n",
        "                overlap_details[district_name] = overlap_length\n",
        "\n",
        "                # Track the district with maximum overlap\n",
        "                if overlap_length > max_overlap_length:\n",
        "                    max_overlap_length = overlap_length\n",
        "                    best_district = district_name\n",
        "\n",
        "        # Build result record\n",
        "        record_dict = record.to_dict()\n",
        "        record_dict['actual_district'] = best_district if best_district else 'Outside any district'\n",
        "        record_dict['is_in_correct_district'] = (assigned_district == best_district) if best_district else False\n",
        "        record_dict['overlap_length'] = max_overlap_length\n",
        "        record_dict['total_road_length'] = road_geom.length\n",
        "        record_dict['overlap_percentage'] = (max_overlap_length / road_geom.length * 100) if road_geom.length > 0 else 0\n",
        "\n",
        "        results.append(record_dict)\n",
        "\n",
        "    # --- 4. Create Result GeoDataFrame ---\n",
        "    validated_gdf = gpd.GeoDataFrame(results, crs=records_gdf.crs)\n",
        "\n",
        "    return validated_gdf"
      ],
      "metadata": {
        "id": "k4X92JXViWTE"
      },
      "id": "k4X92JXViWTE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "districts_gdf = gpd.GeoDataFrame(dis, crs=\"EPSG:4326\")\n",
        "input_tprop = tprop_df2.copy()\n",
        "records_gdf = gpd.GeoDataFrame(input_tprop, crs=\"EPSG:4326\")"
      ],
      "metadata": {
        "id": "9pRbJij3jtX1"
      },
      "id": "9pRbJij3jtX1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tprop.shape[0]"
      ],
      "metadata": {
        "id": "zdKfqIwKqdgA"
      },
      "id": "zdKfqIwKqdgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records_gdf.shape[0]"
      ],
      "metadata": {
        "id": "kA-Nn6fDj5yg"
      },
      "id": "kA-Nn6fDj5yg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validated_records = validate_record_locations(records_gdf, districts_gdf)"
      ],
      "metadata": {
        "id": "AjQVuxQ9jsVA"
      },
      "id": "AjQVuxQ9jsVA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiures_test3 = validated_records[validated_records['is_in_correct_district'] == False]\n",
        "faiures_test3"
      ],
      "metadata": {
        "id": "k8ZYLOXdmEjB"
      },
      "id": "k8ZYLOXdmEjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiures_test3.to_excel('/content/drive/MyDrive/Colab/Capstone 1/faiures_test3.xlsx')"
      ],
      "metadata": {
        "id": "lDkomLa0D4Hv"
      },
      "id": "lDkomLa0D4Hv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validated_records.to_excel('/content/drive/MyDrive/Colab/Capstone 1/fix.xlsx')"
      ],
      "metadata": {
        "id": "K_DyD2w3eYsA"
      },
      "id": "K_DyD2w3eYsA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}